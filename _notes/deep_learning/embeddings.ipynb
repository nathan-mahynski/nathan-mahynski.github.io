{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install fastai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import fastai\n",
        "\n",
        "import sklearn\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "IJFCNepMxQ-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dict(\n",
        "    torch=torch.__version__, \n",
        "    fastai=fastai.__version__, \n",
        "    pandas=pd.__version__, \n",
        "    numpy=np.__version__))"
      ],
      "metadata": {
        "id": "ZNyL9b2ZzoHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings\n",
        "\n",
        "Embeddings are a critical tool for making neural networks efficient, especially when it comes to tabular data.  Embeddings allow NN to be as powerful and tree ensemble methods for tabular data.  \n",
        "\n",
        "When a table contains raw data (text or images) or have very high cardinality categorical features it is generally recommended to use NN instead of random forests, for example.\n",
        "\n",
        "An embedding may be thought of as a combination of one-hot-encoding + dimensionality reduction."
      ],
      "metadata": {
        "id": "51MulkwgxRkd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R10zUwuzQx8Y"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhSsqRwTwDEm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko9tF_wKwDEn"
      },
      "source": [
        "# Collaborative Filtering\n",
        "\n",
        "Collaborative filtering uses embeddings to find latent factors connecting categorical or labeled inputs and outputs (for example, usernames vs. movie titles).  The hypothesis is that there is a structured latent space defined by these embeddings in which similar points reflect similar users or movies."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probabilistic Matrix Factorization Approach\n",
        "\n",
        "This is from [\"Deep learning for coders with fastai & pytorch\"](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527) (Chapter 8).\n",
        "\n",
        "Here we are trying to find the latent factors controlling the connection between users and the movies the like.\n",
        "\n",
        "We will use a very simple metric (dot product) to define matches / similarity. This approach to collaborative filtering is called [probabilistic matrix factorization](https://towardsdatascience.com/probabilistic-matrix-factorization-b7852244a321).\n",
        "\n",
        "Here is a nice introduction from [google](https://developers.google.com/machine-learning/recommendation/collaborative/basics).  One thing google points out is that you can improve things by [weighting observed](https://developers.google.com/machine-learning/recommendation/collaborative/matrix) entries differently that unobserved ones during training (which will make sense soon).  The example below is as simpler implementation whcih does not do this.\n",
        "\n",
        "Some [limitations](https://developers.google.com/machine-learning/recommendation/dnn/softmax), which can be overcome by using NN (next section), include:\n",
        "> 1. This only works on the training data - you cannot make predictions on new, unseen data.  In principle, if you had another model (e.g., NN) to predict the user embedding based on other features you should be able to use the filtering model to make predictions, but this adds another layer - using a NN directly essentially accomplishes the same thing in one step.\n",
        "> 2. \"Popular items tend to be recommended for everyone, especially when using dot product as a similarity measure.\" User-specific interests are not well-tailored."
      ],
      "metadata": {
        "id": "XS7bsvlbxk64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastai.collab import *\n",
        "from fastai.tabular.all import *"
      ],
      "metadata": {
        "id": "Aa9v6me7w8Uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = untar_data(URLs.ML_100k)\n",
        "ratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None, names=['user', 'movie', 'rating', 'timestamp'])\n",
        "movies = pd.read_csv(path/'u.item', delimiter='|', encoding='latin-1', usecols=(0,1), names=('movie', 'title'), header=None)"
      ],
      "metadata": {
        "id": "lGvFj2Cgw659"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movies.head()"
      ],
      "metadata": {
        "id": "oafnXYZRxAAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings.head()"
      ],
      "metadata": {
        "id": "zziFz2nJxiF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings = ratings.merge(movies) # merge based on movie (common column)"
      ],
      "metadata": {
        "id": "yk9JbRZyySfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64) # Make a dataloader from the dataframe\n",
        "dls.show_batch()"
      ],
      "metadata": {
        "id": "op23pJk1yvp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_users = len(dls.classes['user'])\n",
        "n_movies = len(dls.classes['title'])\n",
        "n_factors = 5 # Arbitrary choice for this example - in reality, this is a hyperparameter for tuning\n",
        "\n",
        "# Let's set up the latent factors we are going to learn\n",
        "user_factors = torch.randn(n_users, n_factors)\n",
        "movie_factors = torch.randn(n_movies, n_factors)"
      ],
      "metadata": {
        "id": "vE1w_6F0zCYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "?sigmoid_range"
      ],
      "metadata": {
        "id": "iGHSvEAG-ATM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = sigmoid_range(torch.tensor([-100,0,1,2,3,4,5,100]), 1, 10)"
      ],
      "metadata": {
        "id": "7M0AsDKn-Bhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lower = 1\n",
        "upper = 10\n",
        "x = torch.tensor(np.linspace(-10, +10, 1000))\n",
        "y = sigmoid_range(x, lower, upper) # This creates a response variable that is stretched out\n",
        "\n",
        "plt.plot(x,y)"
      ],
      "metadata": {
        "id": "GLbfi2up-Kmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's do this using fastai's built in Embedding module\n",
        "\n",
        "class DotProductBias(Module):\n",
        "  def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n",
        "    self.user_factors = Embedding(n_users, n_factors)\n",
        "    self.user_bias = Embedding(n_users, 1)\n",
        "    self.movie_factors = Embedding(n_movies, n_factors)\n",
        "    self.movie_bias = Embedding(n_movies, 1)\n",
        "    self.y_range = y_range\n",
        "\n",
        "  def forward(self, x):\n",
        "    users = self.user_factors(x[:,0])\n",
        "    movies = self.movie_factors(x[:,1])\n",
        "    # A dot product reflects the similarity between 2 vectors so is a convenient \"matching\" approach\n",
        "    res = (users * movies).sum(dim=1, keepdim=True) + self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n",
        "    return sigmoid_range(res, *self.y_range)\n",
        "\n",
        "model = DotProductBias(n_users, n_movies, 50)\n",
        "learn = Learner(dls, model, loss_func=MSELossFlat()) "
      ],
      "metadata": {
        "id": "pHoAZHBD_BBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.fit_one_cycle(5, 5e-3, wd=0.1 )# Use weight decay (l2 norm) to regularize"
      ],
      "metadata": {
        "id": "iQVNj_0a_gLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To be a little more transparent we can make our own \"Embedding\" module\n",
        "def create_params(size):\n",
        "  # nn.Parameter tells pytorch this is a trainable parameter\n",
        "  # This also randomizes them\n",
        "  return nn.Parameter(torch.zeros(*size).normal_(0, 0.01))\n",
        "\n",
        "class ManualDotProductBias(Module):\n",
        "  def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n",
        "    self.user_factors = create_params([n_users, n_factors])\n",
        "    self.user_bias = create_params([n_users])\n",
        "    self.movie_factors = create_params([n_movies, n_factors])\n",
        "    self.movie_bias = create_params([n_movies])\n",
        "    self.y_range = y_range\n",
        "\n",
        "  def forward(self, x):\n",
        "    users = self.user_factors[x[:,0]]\n",
        "    movies = self.movie_factors[x[:,1]]\n",
        "    # A dot product reflects the similarity between 2 vectors so is a convenient \"matching\" approach\n",
        "    res = (users * movies).sum(dim=1) + self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]\n",
        "    return sigmoid_range(res, *self.y_range)\n",
        "\n",
        "model = ManualDotProductBias(n_users, n_movies, 50)\n",
        "learn = Learner(dls, model, loss_func=MSELossFlat(), wd=0.1) # Use weight decay (l2 norm) to regularize"
      ],
      "metadata": {
        "id": "1TFXov4DB8bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.fit_one_cycle(5, 5e-3, wd=0.1) # Use weight decay (l2 norm) to regularize"
      ],
      "metadata": {
        "id": "Ksechj8CDU5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The bias tells us what movies are just \"bad\" - i.e., even if the dot product is high (movie well matched to user's general taste), a low bias means they probably still won't like it\n",
        "# Conversely, high biases can show movies that are very \"good\" and even users who don't normally like a certain type of movie will probably still enjoy it.\n",
        "\n",
        "movie_bias = learn.model.movie_bias.squeeze()\n",
        "idxs = movie_bias.argsort()[:5]\n",
        "bad_movies = [dls.classes['title'][i] for i in idxs]\n",
        "\n",
        "idxs = movie_bias.argsort(descending=True)[:5]\n",
        "good_movies = [dls.classes['title'][i] for i in idxs]"
      ],
      "metadata": {
        "id": "9EqYhlM4E2v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bad_movies"
      ],
      "metadata": {
        "id": "1qggloxBGAv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "good_movies"
      ],
      "metadata": {
        "id": "KQlJ1K28GHpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can look at the latent space\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "factors = learn.model.movie_factors.detach().numpy()\n",
        "look_at = 20 # Look at the \"best\" N movies\n",
        "\n",
        "idxs_best = movie_bias.argsort(descending=True)[:look_at]\n",
        "mask = np.array([True if i in idxs_best else False for i in range(n_movies)])\n",
        "low_d_best = pca.fit_transform(factors[mask])\n",
        "\n",
        "idxs_worst = movie_bias.argsort(descending=False)[:look_at]\n",
        "mask = np.array([True if i in idxs_worst else False for i in range(n_movies)])\n",
        "low_d_worst = pca.fit_transform(factors[mask])"
      ],
      "metadata": {
        "id": "sJGfm74pHqBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(low_d_best[:,0], low_d_best[:,1], 'go')\n",
        "for i in range(len(low_d_best)):\n",
        "  plt.text(low_d_best[i,0], low_d_best[i,1], dls.classes['title'][idxs_best[i]])\n",
        "\n",
        "plt.plot(low_d_worst[:,0], low_d_worst[:,1], 'ro')\n",
        "for i in range(len(low_d_worst)):\n",
        "  plt.text(low_d_worst[i,0], low_d_worst[i,1], dls.classes['title'][idxs_worst[i]])"
      ],
      "metadata": {
        "id": "VOLuWlaeHxBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fastai does the same thing\n",
        "\n",
        "learn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\n",
        "learn.fit_one_cycle(5, 5e-3, wd=0.1)"
      ],
      "metadata": {
        "id": "av9DQqnGJuVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.model"
      ],
      "metadata": {
        "id": "NNppHedNNoEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_bias = learn.model.i_bias.weight.squeeze()\n",
        "idxs = movie_bias.argsort(descending=True)[:5]\n",
        "[dls.classes['title'][i] for i in idxs]"
      ],
      "metadata": {
        "id": "w6MdDY6fNpu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importantly, the latent space can be used to define distances, for example - similar things should be close in this space (we hope), so we can define recommendations, or authentication/predictions, based on similarity to a known exemplar."
      ],
      "metadata": {
        "id": "uR1HrJmBN8w0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Neural Network Approach\n",
        "\n",
        "This is from [\"Deep learning for coders with fastai & pytorch\"](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527) (Chapter 8).\n",
        "\n",
        "Now, we will use a neural network to predict the latent similarity.  This is achieved by concatenating the factors of the movies and the users to create a single input, then a NN predicts the \"degree of recommendation / compatability\" based on that net input.\n",
        "\n",
        "Google also has a nice example [here](https://developers.google.com/machine-learning/recommendation/dnn/softmax)."
      ],
      "metadata": {
        "id": "QS4BHr5BOeo2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r8AMynGdOIsq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}