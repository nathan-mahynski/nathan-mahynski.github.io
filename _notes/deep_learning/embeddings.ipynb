{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install fastai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import fastai\n",
        "\n",
        "import sklearn\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "IJFCNepMxQ-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dict(\n",
        "    torch=torch.__version__, \n",
        "    fastai=fastai.__version__, \n",
        "    pandas=pd.__version__, \n",
        "    numpy=np.__version__))"
      ],
      "metadata": {
        "id": "ZNyL9b2ZzoHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings\n",
        "\n",
        "Embeddings are a critical tool for making neural networks efficient, especially when it comes to tabular data.  Embeddings allow NN to be as powerful and tree ensemble methods for tabular data.  \n",
        "\n",
        "When a table contains raw data (text or images) or have very high cardinality categorical features it is generally recommended to use NN instead of random forests, for example.\n",
        "\n",
        "However, for many cases of categorical variables, there is no evidence that OHE actually improves the performance of such tree ensembles, so you can just keep the catogerical variable as a raw input.  For other models, such as those discuss here, you cannot do that.\n",
        "\n",
        "**An embedding may be thought of as a combination of one-hot-encoding + dimensionality reduction.**\n",
        "\n",
        "[Fastai](https://docs.fast.ai/) has a [tool for heuristically suggesting](https://docs.fast.ai/tabular.model.html#get_emb_sz) the best size for your embeddings. This is just a suggestion, but a good starting point.\n",
        "~~~code\n",
        "get_emb_sz(my_dataloader)\n",
        "~~~\n",
        "\n",
        "An embedding layer is exactly equivalent to placing an ordinary linear layer (NN) after a one-hot-encoding layer or transformation. [Entity embedding](https://paperswithcode.com/paper/entity-embeddings-of-categorical-variables) is essentially just a separate NN after a OHE transformation for each categorical variable.  This transforms each category (entity) into its own embedding."
      ],
      "metadata": {
        "id": "51MulkwgxRkd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aCNV25f8ktsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qTU81SXfktuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tzw4o2-Oktwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aoKJVoUYktys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko9tF_wKwDEn"
      },
      "source": [
        "# Collaborative Filtering\n",
        "\n",
        "Collaborative filtering uses embeddings to find latent factors connecting categorical or labeled inputs and outputs (for example, usernames vs. movie titles).  The hypothesis is that there is a structured latent space defined by these embeddings in which similar points reflect similar users or movies.\n",
        "\n",
        "[Fastai](https://docs.fast.ai/) has a lightweight, fast [collab_learner](https://docs.fast.ai/collab.html) which is a good tool for a first pass if you don't want to dive too deeply into the details."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probabilistic Matrix Factorization Approach\n",
        "\n",
        "This is from [\"Deep learning for coders with fastai & pytorch\"](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527) (Chapter 8).\n",
        "\n",
        "Here we are trying to find the latent factors controlling the connection between users and the movies the like.\n",
        "\n",
        "We will use a very simple metric (dot product) to define matches / similarity. This approach to collaborative filtering is called [probabilistic matrix factorization](https://towardsdatascience.com/probabilistic-matrix-factorization-b7852244a321). The basic assumption is that the response (here, movie rating from 1-5 is a product of the user's characteristic vector with the movie's characteristic vector - the vectors are the embeddings!).\n",
        "\n",
        "Here is a nice introduction from [google](https://developers.google.com/machine-learning/recommendation/collaborative/basics).  One thing google points out is that you can improve things by [weighting observed](https://developers.google.com/machine-learning/recommendation/collaborative/matrix) entries differently that unobserved ones during training (which will make sense soon).  The example below is as simpler implementation whcih does not do this.\n",
        "\n",
        "Some [limitations](https://developers.google.com/machine-learning/recommendation/dnn/softmax), which can be overcome by using NN (next section), include:\n",
        "> 1. This only works on the training data - you cannot make predictions on new, unseen data.  In principle, if you had another model (e.g., NN) to predict the user embedding based on other features you should be able to use the filtering model to make predictions, but this adds another layer - using a NN directly essentially accomplishes the same thing in one step.\n",
        "> 2. \"Popular items tend to be recommended for everyone, especially when using dot product as a similarity measure.\" User-specific interests are not well-tailored."
      ],
      "metadata": {
        "id": "XS7bsvlbxk64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastai.collab import *\n",
        "from fastai.tabular.all import *"
      ],
      "metadata": {
        "id": "Aa9v6me7w8Uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = untar_data(URLs.ML_100k)\n",
        "ratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None, names=['user', 'movie', 'rating', 'timestamp'])\n",
        "movies = pd.read_csv(path/'u.item', delimiter='|', encoding='latin-1', usecols=(0,1), names=('movie', 'title'), header=None)"
      ],
      "metadata": {
        "id": "lGvFj2Cgw659"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movies.head()"
      ],
      "metadata": {
        "id": "oafnXYZRxAAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings.head()"
      ],
      "metadata": {
        "id": "zziFz2nJxiF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    ratings['rating'].min(),\n",
        "    ratings['rating'].max())"
      ],
      "metadata": {
        "id": "J-42XZ_-g8RB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings = ratings.merge(movies) # merge based on movie (common column)"
      ],
      "metadata": {
        "id": "yk9JbRZyySfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64) # Make a dataloader from the dataframe\n",
        "dls.show_batch()"
      ],
      "metadata": {
        "id": "op23pJk1yvp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_users = len(dls.classes['user'])\n",
        "n_movies = len(dls.classes['title'])"
      ],
      "metadata": {
        "id": "vE1w_6F0zCYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lower = 1\n",
        "upper = 10\n",
        "x = torch.tensor(np.linspace(-10, +10, 1000))\n",
        "y = sigmoid_range(x, lower, upper) # This creates a response variable that is stretched out\n",
        "\n",
        "plt.plot(x,y)"
      ],
      "metadata": {
        "id": "GLbfi2up-Kmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's do this using fastai's built in Embedding module\n",
        "\n",
        "# We are going to predict the user ratings (1-5) - heuristically increasing the scale (y_range) a bit seems to help\n",
        "\n",
        "class DotProductBias(Module):\n",
        "  def __init__(self, n_users, n_movies, n_factors, y_range=(0, 5.5)):\n",
        "    self.user_factors = Embedding(n_users, n_factors)\n",
        "    self.user_bias = Embedding(n_users, 1)\n",
        "    self.movie_factors = Embedding(n_movies, n_factors)\n",
        "    self.movie_bias = Embedding(n_movies, 1)\n",
        "    self.y_range = y_range\n",
        "\n",
        "  def forward(self, x):\n",
        "    users = self.user_factors(x[:,0])\n",
        "    movies = self.movie_factors(x[:,1])\n",
        "    # A dot product reflects the similarity between 2 vectors so is a convenient \"matching\" approach\n",
        "    res = (users * movies).sum(dim=1, keepdim=True) + self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n",
        "    return sigmoid_range(res, *self.y_range)\n",
        "\n",
        "model = DotProductBias(n_users, n_movies, 50)\n",
        "learn = Learner(dls, model, loss_func=MSELossFlat()) "
      ],
      "metadata": {
        "id": "pHoAZHBD_BBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.fit_one_cycle(5, 5e-3, wd=0.1 )# Use weight decay (l2 norm) to regularize"
      ],
      "metadata": {
        "id": "iQVNj_0a_gLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To be a little more transparent we can make our own \"Embedding\" module\n",
        "def create_params(size):\n",
        "  # nn.Parameter tells pytorch this is a trainable parameter\n",
        "  # This also randomizes them\n",
        "  return nn.Parameter(torch.zeros(*size).normal_(0, 0.01))\n",
        "\n",
        "class ManualDotProductBias(Module):\n",
        "  def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n",
        "    self.user_factors = create_params([n_users, n_factors])\n",
        "    self.user_bias = create_params([n_users])\n",
        "    self.movie_factors = create_params([n_movies, n_factors])\n",
        "    self.movie_bias = create_params([n_movies])\n",
        "    self.y_range = y_range\n",
        "\n",
        "  def forward(self, x):\n",
        "    users = self.user_factors[x[:,0]]\n",
        "    movies = self.movie_factors[x[:,1]]\n",
        "    # A dot product reflects the similarity between 2 vectors so is a convenient \"matching\" approach\n",
        "    res = (users * movies).sum(dim=1) + self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]\n",
        "    return sigmoid_range(res, *self.y_range)\n",
        "\n",
        "model = ManualDotProductBias(n_users, n_movies, 50)\n",
        "learn = Learner(dls, model, loss_func=MSELossFlat(), wd=0.1) # Use weight decay (l2 norm) to regularize"
      ],
      "metadata": {
        "id": "1TFXov4DB8bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.fit_one_cycle(5, 5e-3, wd=0.1) # Use weight decay (l2 norm) to regularize"
      ],
      "metadata": {
        "id": "Ksechj8CDU5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The bias tells us what movies are just \"bad\" - i.e., even if the dot product is high (movie well matched to user's general taste), a low bias means they probably still won't like it\n",
        "# Conversely, high biases can show movies that are very \"good\" and even users who don't normally like a certain type of movie will probably still enjoy it.\n",
        "\n",
        "movie_bias = learn.model.movie_bias.squeeze()\n",
        "idxs = movie_bias.argsort()[:5]\n",
        "bad_movies = [dls.classes['title'][i] for i in idxs]\n",
        "\n",
        "idxs = movie_bias.argsort(descending=True)[:5]\n",
        "good_movies = [dls.classes['title'][i] for i in idxs]"
      ],
      "metadata": {
        "id": "9EqYhlM4E2v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bad_movies"
      ],
      "metadata": {
        "id": "1qggloxBGAv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "good_movies"
      ],
      "metadata": {
        "id": "KQlJ1K28GHpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can look at the latent space\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "factors = learn.model.movie_factors.detach().numpy()\n",
        "look_at = 20 # Look at the \"best\" N movies\n",
        "\n",
        "idxs_best = movie_bias.argsort(descending=True)[:look_at]\n",
        "mask = np.array([True if i in idxs_best else False for i in range(n_movies)])\n",
        "low_d_best = pca.fit_transform(factors[mask])\n",
        "\n",
        "idxs_worst = movie_bias.argsort(descending=False)[:look_at]\n",
        "mask = np.array([True if i in idxs_worst else False for i in range(n_movies)])\n",
        "low_d_worst = pca.fit_transform(factors[mask])"
      ],
      "metadata": {
        "id": "sJGfm74pHqBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(low_d_best[:,0], low_d_best[:,1], 'go')\n",
        "for i in range(len(low_d_best)):\n",
        "  plt.text(low_d_best[i,0], low_d_best[i,1], dls.classes['title'][idxs_best[i]])\n",
        "\n",
        "plt.plot(low_d_worst[:,0], low_d_worst[:,1], 'ro')\n",
        "for i in range(len(low_d_worst)):\n",
        "  plt.text(low_d_worst[i,0], low_d_worst[i,1], dls.classes['title'][idxs_worst[i]])"
      ],
      "metadata": {
        "id": "VOLuWlaeHxBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fastai does the same thing\n",
        "\n",
        "learn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\n",
        "learn.fit_one_cycle(5, 5e-3, wd=0.1)"
      ],
      "metadata": {
        "id": "av9DQqnGJuVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.model"
      ],
      "metadata": {
        "id": "NNppHedNNoEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_bias = learn.model.i_bias.weight.squeeze()\n",
        "idxs = movie_bias.argsort(descending=True)[:5]\n",
        "[dls.classes['title'][i] for i in idxs]"
      ],
      "metadata": {
        "id": "w6MdDY6fNpu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importantly, the latent space can be used to define distances, for example - similar things should be close in this space (we hope), so we can define recommendations, or authentication/predictions, based on similarity to a known exemplar."
      ],
      "metadata": {
        "id": "uR1HrJmBN8w0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Neural Network Approach\n",
        "\n",
        "This is from [\"Deep learning for coders with fastai & pytorch\"](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527) (Chapter 8).\n",
        "\n",
        "Now, we will use a neural network to predict the latent similarity.  This is achieved by concatenating the factors of the movies and the users to create a single input, then a NN predicts the \"degree of recommendation / compatability\" based on that net input.\n",
        "\n",
        "Google also has a nice example [here](https://developers.google.com/machine-learning/recommendation/dnn/softmax) which does multiclass prediction (probability of liking different videos) - this example predicts a single scalar (user rating) so we use MSELoss instead of categorical cross entropy, for example, but the point is this can be adapted.\n",
        "\n",
        "In addition to overcoming the shortcomings of the PMF approach above, NN also enable you to concatenate other continuous variables with the input to these layers to create [wide and deep NN](https://medium.com/analytics-vidhya/wide-deep-learning-for-recommender-systems-dc99094fc291) as explained in this [paper](https://arxiv.org/abs/1606.07792) from google. Also see [here](https://developers.google.com/machine-learning/recommendation/dnn/softmax)."
      ],
      "metadata": {
        "id": "QS4BHr5BOeo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastai.collab import *\n",
        "from fastai.tabular.all import *"
      ],
      "metadata": {
        "id": "r8AMynGdOIsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = untar_data(URLs.ML_100k)\n",
        "ratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None, names=['user', 'movie', 'rating', 'timestamp'])\n",
        "movies = pd.read_csv(path/'u.item', delimiter='|', encoding='latin-1', usecols=(0,1), names=('movie', 'title'), header=None)\n",
        "ratings = ratings.merge(movies) # merge based on movie (common column)\n",
        "dls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64) # Make a dataloader from the dataframe"
      ],
      "metadata": {
        "id": "PghXFoo4bz0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dls.classes.keys()"
      ],
      "metadata": {
        "id": "ran3jhLScqMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_emb_sz(dls) # fastai has a heuristic tool to recommend the size of your embedding for each 'class'"
      ],
      "metadata": {
        "id": "kzwRMS21bz2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "??get_emb_sz"
      ],
      "metadata": {
        "id": "uEvShz2ec_ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CollabNN(Module):\n",
        "  def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100):\n",
        "    self.user_factors = Embedding(*user_sz)\n",
        "    self.item_factors = Embedding(*item_sz)\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(user_sz[1] + item_sz[1], n_act),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(n_act, 1) # Output a single scalar = user rating\n",
        "    )\n",
        "    self.y_range = y_range\n",
        "\n",
        "  def forward(self, x):\n",
        "    embs = self.user_factors(x[:,0]), self.item_factors(x[:,1])\n",
        "    x = self.layers(torch.cat(embs, dim=1))\n",
        "    return sigmoid_range(x, *self.y_range)"
      ],
      "metadata": {
        "id": "W6ZJ2w0NcQlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CollabNN(*get_emb_sz(dls))"
      ],
      "metadata": {
        "id": "QK_e5YgXccXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dls.items"
      ],
      "metadata": {
        "id": "3TlUkpqbgm62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn = Learner(dls, model, loss_func=MSELossFlat())\n",
        "learn.fit_one_cycle(5, 5e-3, wd=0.01)"
      ],
      "metadata": {
        "id": "9Tjc2R5rd2S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fastai has a shortcut function for this which uses get_emb_sz automatically\n",
        "\n",
        "learn = collab_learner(dls, \n",
        "                       use_nn=True, # Use NN instead of PMF\n",
        "                       y_range=(0,5.5), \n",
        "                       layers=[100, 50] # You can also adjust the size of the NN automatically\n",
        "                       )\n",
        "learn.fit_one_cycle(5, 5e-3, wd=0.1)"
      ],
      "metadata": {
        "id": "QKH4MW81fO7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LGImZ0C8f40a"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}