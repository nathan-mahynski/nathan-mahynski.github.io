{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "41984ba2",
      "metadata": {
        "id": "41984ba2"
      },
      "source": [
        "# tl;dr\n",
        "\n",
        "The transformer architecture was introduced in the paper [\"Attention is all you need\"](https://arxiv.org/abs/1706.03762), however attention mechanisms existed before this. Attention is a method that allows the model to focus on relevant parts of the input (context) in arbitrary (non-local) ways; high capacity models using attention mechanisms, trained with sufficient data, have surpassed recurrent neural network architectures as state-of-the-art (LSTM, etc. and even convolutions, too) on many tasks. This is largely due to the fact that transformers can be trained in parallel allowing them to use much larger training datasets than other recurrent models.\n",
        "\n",
        "Attention makes a direct connection between points in the input (e.g, sequence); as a result you can [view attention essentially like a graph](https://graphdeeplearning.github.io/post/transformers-are-gnns/) where each part of the input is connected to all others and the weight of each edge determines the \"strength\" (how much attention to pay) of the interaction. Thus, transformers are a special case of graph neural networks!\n",
        "\n",
        "Good Tutorials:\n",
        "* [fast.ai lesson](https://course.fast.ai/Lessons/lesson24.html)\n",
        "* [UvA tutorial](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3787dffe",
      "metadata": {
        "id": "3787dffe"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import torch\n",
        "import deepchem\n",
        "import sklearn\n",
        "import rdkit\n",
        "import simpletransformers\n",
        "import transformers\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from IPython.display import YouTubeVideo\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import watermark\n",
        "%load_ext watermark\n",
        "%watermark -t -m -v --iversions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56f3b809",
      "metadata": {
        "heading_collapsed": true,
        "id": "56f3b809"
      },
      "source": [
        "# Attention Mechanisms"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "080d5e6f",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "080d5e6f"
      },
      "source": [
        "## Notes from CH. 16 in [Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow](https://github.com/ageron/handson-ml2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10224041",
      "metadata": {
        "hidden": true,
        "id": "10224041"
      },
      "source": [
        "Attention started with [this paper](https://arxiv.org/abs/1409.0473) in **2014** where the decoder was able to focus on different words in the input dynamically, and as needed.  Thus, the \"length of the path\" from an input word to its translation was much shorter than in other recurrent models. This is called \"concatenative\" or \"Bahdanau\" attention and uses a neural net to do \"alignment.\"\n",
        "\n",
        "[\"Luong\", or \"multiplicative\", attention](https://arxiv.org/abs/1508.04025) was proposed the following year in **2015** - the goal of the attention mechanism is to measure the similarity between one of the encoder's outputs and the decoder's previous state, the authors simply proposed a dot product of these vectors, which is the most commonly used type attention today. It is usually faster to compute and performs better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97af9f83",
      "metadata": {
        "hidden": true,
        "id": "97af9f83"
      },
      "outputs": [],
      "source": [
        "%%HTML\n",
        "<img src=\"https://uvadlc-notebooks.readthedocs.io/en/latest/_images/scaled_dot_product_attn.svg\"\n",
        "width=\"200\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7625ddba",
      "metadata": {
        "hidden": true,
        "id": "7625ddba"
      },
      "source": [
        "They also proposed a variant where the inputs are first sent through a linear transformation before the dot product is taken called the \"general dot product approach\" - I seem to see this a lot since it adds fittable parameters. Below is a visualization from the \"multi-head\" attention mechanism using this approach. (Note **there is no activation function** on any of the linear layers applied below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a931efb",
      "metadata": {
        "hidden": true,
        "id": "0a931efb"
      },
      "outputs": [],
      "source": [
        "%%HTML\n",
        "<img src=\"https://uvadlc-notebooks.readthedocs.io/en/latest/_images/multihead_attention.svg\"\n",
        "width=\"200\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9efe2955",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "9efe2955"
      },
      "source": [
        "## YouTube Introductions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11d85eb0",
      "metadata": {
        "hidden": true,
        "id": "11d85eb0"
      },
      "outputs": [],
      "source": [
        "YouTubeVideo(\"yGTUuEx3GkA\", width=400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7502b77e",
      "metadata": {
        "hidden": true,
        "id": "7502b77e"
      },
      "outputs": [],
      "source": [
        "YouTubeVideo(\"tIvKXrEDMhk\", width=400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0959ada6",
      "metadata": {
        "hidden": true,
        "id": "0959ada6"
      },
      "outputs": [],
      "source": [
        "YouTubeVideo(\"23XUv0T9L5c\", width=400)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fc0834e",
      "metadata": {
        "heading_collapsed": true,
        "id": "6fc0834e"
      },
      "source": [
        "# Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6d8ceb9",
      "metadata": {
        "hidden": true,
        "id": "f6d8ceb9"
      },
      "source": [
        "In **2017**, the [\"Attention is all you need\"](https://arxiv.org/abs/1706.03762) paper was published, which showed that attention mechanisms alone (without any recurrent layers) is sufficient to handle neural machine translation tasks (NMT) using the \"transformer\" architecture.  Since then, transformers achieved SOTA performance in many areas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0604edf",
      "metadata": {
        "hidden": true,
        "id": "d0604edf"
      },
      "outputs": [],
      "source": [
        "%%HTML\n",
        "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\"\n",
        "width=\"400\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a795eace",
      "metadata": {
        "hidden": true,
        "id": "a795eace"
      },
      "source": [
        "Encoder\n",
        "---\n",
        "The lefthand side is the encoder - inputs of size (batch size, max input sentence length)\n",
        "The encoder outputs each token or word as an embedding with K dimensions - output has size (batch size, max input sentence length, K)\n",
        "In the original architecture N = 6, so there are 6 stacks of the encoder part.\n",
        "\n",
        "Feedforward parts are added in each block to help \"post process\" the new information.  Essentially, ecah block keeps updating the information to provide better context.\n",
        "\n",
        "Decoder\n",
        "---\n",
        "The righthand side is the decoder - it accepts the original sentence (shifted to the right by 1 place) plus the encoder output and gives a probability of the next word. Output of size (batch size, max input sentence length, vocabulary length).\n",
        "As a result, the model is called repeatedly, each time predicting the next word in the sentence.\n",
        "Note that the encoder's final output is actually fed to each of the N decoder stacks.\n",
        "\n",
        "The decoder's Masked MH Attention is used so that each word is only allowed to attend to the words that came before it.\n",
        "BERT uses a similar architecture to GPT-2 but dropped these Masked MH Attention layers from that model to make it \"bidirectional\" (the \"B\" in BERT).\n",
        "\n",
        "Attention\n",
        "---\n",
        "The attention mechanism is essentially Luong style, but contains a scale normalization constant.\n",
        "\n",
        "The K, Q, V and the attenion operations are explained nicely (with examples) in:\n",
        "1. [UvA tutorial](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)\n",
        "2. [dmol.pub](https://dmol.pub/dl/attention.html)\n",
        "\n",
        "Multiheaded attention allows attention mechanism to project the embeddings in different contexts to interpret different parts in different ways.\n",
        "\n",
        "Other Notes\n",
        "---\n",
        "LayerNorms are used throughput to keep the layers values on the same order as their inputs (since skip connections are used).\n",
        "\n",
        "Positional encodings (PE) are dense vectors that are added to the embeddings of each token at the same position to impart information about where the token occurs in the sentence.  This is necessary because attention is basically a graph and has no concept of order. See the [UvA tutorial](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html) for code and examples.\n",
        "\n",
        "Pre-training allows the encodings (output of encoder blocks) to be very helpful with transfer learning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f8b398c",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "7f8b398c"
      },
      "source": [
        "## Architecture and Discussions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a9e53d2",
      "metadata": {
        "hidden": true,
        "scrolled": false,
        "id": "9a9e53d2"
      },
      "outputs": [],
      "source": [
        "YouTubeVideo(\"iDulhoQ2pro\", width=400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cca90fc1",
      "metadata": {
        "hidden": true,
        "id": "cca90fc1"
      },
      "outputs": [],
      "source": [
        "YouTubeVideo(\"EXNBy8G43MM\", width=400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e2449db",
      "metadata": {
        "hidden": true,
        "id": "8e2449db"
      },
      "outputs": [],
      "source": [
        "YouTubeVideo(\"-QH8fRhqFHM\", width=400)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aed6fc4e",
      "metadata": {
        "hidden": true,
        "id": "aed6fc4e"
      },
      "source": [
        "Also see [blog](https://jalammar.github.io/illustrated-transformer/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f11c3c7c",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "f11c3c7c"
      },
      "source": [
        "## Explainability"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c0adff7",
      "metadata": {
        "hidden": true,
        "id": "5c0adff7"
      },
      "source": [
        "Attention probabilities can be used to visualize where the model is attending to given an input token.  However, as warned [here](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html):\n",
        "\n",
        "> \"This helps us in understanding, and in a sense, explaining the model. However, the attention probabilities should be interpreted with a grain of salt as it does not necessarily reflect the true interpretation of the model (there is a series of papers about this, including [Attention is not Explanation](https://arxiv.org/abs/1902.10186) and [Attention is not not Explanation](https://arxiv.org/abs/1908.04626)\"\n",
        "\n",
        "Basically, this boils down to a [similar issue with using Grad-CAM to explain CNNs](https://arxiv.org/abs/2011.08891) - if you have a GAP layer + 1 Dense layer at the end, you can take a gradient wrt of the scores to reliably highlight parts of the image that contribute positively to a classification.  However, with other architectures there is no guarantee because the mathematical transformations in the deep layers at the end (head) are not being explained.\n",
        "\n",
        "This is from: https://github.com/deepchem/deepchem/blob/master/examples/tutorials/Transfer_Learning_With_ChemBERTa_Transformers.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06cb4c7d",
      "metadata": {
        "hidden": true,
        "id": "06cb4c7d"
      },
      "outputs": [],
      "source": [
        "%%javascript\n",
        "require.config({\n",
        "  paths: {\n",
        "      d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.8/d3.min',\n",
        "      jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
        "  }\n",
        "});"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad26b94b",
      "metadata": {
        "hidden": true,
        "id": "ad26b94b"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# !git clone https://github.com/jessevig/bertviz bertviz_repo\n",
        "# sys.path.append('bertviz_repo/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fa7bfbf",
      "metadata": {
        "hidden": true,
        "id": "1fa7bfbf"
      },
      "outputs": [],
      "source": [
        "def call_html():\n",
        "    import IPython\n",
        "    display(IPython.core.display.HTML('''\n",
        "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "        <script>\n",
        "          requirejs.config({\n",
        "            paths: {\n",
        "              base: '/static/base',\n",
        "              \"d3\": \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.8/d3.min\",\n",
        "              jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
        "            },\n",
        "          });\n",
        "        </script>\n",
        "        '''))\n",
        "\n",
        "from bertviz import head_view\n",
        "\n",
        "model_version = 'seyonec/PubChem10M_SMILES_BPE_450k'\n",
        "model = RobertaModel.from_pretrained(model_version, output_attentions=True)\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_version)\n",
        "\n",
        "smiles = \"CCCCC[C@@H](Br)CC\"\n",
        "inputs = tokenizer.encode_plus(smiles, return_tensors='pt')\n",
        "input_ids = inputs['input_ids']\n",
        "attention = model(input_ids)[-1]\n",
        "input_id_list = input_ids[0].tolist() # Batch index 0\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
        "\n",
        "call_html()\n",
        "\n",
        "head_view(attention, tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6fff301",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "a6fff301"
      },
      "source": [
        "## BERTology"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f502f446",
      "metadata": {
        "hidden": true,
        "id": "f502f446"
      },
      "source": [
        "https://huggingface.co/transformers/v3.0.2/bertology.html"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7555ec98",
      "metadata": {
        "heading_collapsed": true,
        "id": "7555ec98"
      },
      "source": [
        "# Examples of transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed772f95",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "ed772f95"
      },
      "source": [
        "## \"Foundational\" models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b9f5c76",
      "metadata": {
        "hidden": true,
        "id": "1b9f5c76"
      },
      "source": [
        "Pre-training transformers enables their encoder blocks to be very useful for fine-tuning on other tasks.\n",
        "\n",
        "[BERT](https://arxiv.org/abs/1810.04805) is a great example.  However, it was shown this model was undertrained and needed some refinement - this is what [RoBERTa](https://arxiv.org/abs/1907.11692) is (just a better way to train BERT models).\n",
        "\n",
        "[ChemBERTa](https://arxiv.org/abs/2010.09885) (also [v2](https://arxiv.org/abs/2209.01712)) is basically the application of RoBERTa principles to train a BERT model on SMILES strings so that its encodings are \"foundational\" - that is, they basically learn the fundamental aspects of chemistry and thus, are very useful for transfer learning applications."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1e2c6d6",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "a1e2c6d6"
      },
      "source": [
        "## Code examples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d5ba09c",
      "metadata": {
        "hidden": true,
        "id": "8d5ba09c"
      },
      "source": [
        "The [transformers](https://huggingface.co/docs/transformers/installation) library by Hugging Face hosts the models and weights for many popular tranformer architectures enabling their use in transfer learning.\n",
        "\n",
        "The [simpletransformers](https://simpletransformers.ai/) tool makes it even easier.\n",
        "\n",
        "Other examples include:\n",
        "\n",
        "* https://dmol.pub/dl/pretraining.html\n",
        "    \n",
        "* https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html\n",
        "\n",
        "* https://github.com/deepchem/deepchem/blob/master/examples/tutorials/Transfer_Learning_With_ChemBERTa_Transformers.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5347b1e2",
      "metadata": {
        "hidden": true,
        "id": "5347b1e2"
      },
      "outputs": [],
      "source": [
        "from simpletransformers.classification import ClassificationModel\n",
        "import pandas as pd, sklearn, matplotlib.pyplot as plt, numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "770f1c0c",
      "metadata": {
        "hidden": true,
        "id": "770f1c0c"
      },
      "outputs": [],
      "source": [
        "soldata = pd.read_csv(\n",
        "    \"https://github.com/whitead/dmol-book/raw/main/data/curated-solubility-dataset.csv\"\n",
        ")\n",
        "\n",
        "N = int(len(soldata) * 0.1)\n",
        "sample = soldata.sample(N, replace=False)\n",
        "train = sample[: int(0.8 * N)]\n",
        "test = sample[int(0.8 * N) :]\n",
        "\n",
        "train_dataset = train[[\"SMILES\", \"Solubility\"]]\n",
        "train_dataset = train_dataset.rename(columns={\"Solubility\": \"labels\", \"SMILES\": \"text\"})\n",
        "test_dataset = test[[\"SMILES\", \"Solubility\"]]\n",
        "test_dataset = test_dataset.rename(columns={\"Solubility\": \"labels\", \"SMILES\": \"text\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd46f581",
      "metadata": {
        "hidden": true,
        "id": "cd46f581"
      },
      "outputs": [],
      "source": [
        "model = ClassificationModel(\n",
        "    \"roberta\",\n",
        "    \"seyonec/ChemBERTa_zinc250k_v2_40k\",\n",
        "    num_labels=1,\n",
        "    args={\n",
        "        \"num_train_epochs\": 5,\n",
        "        \"regression\": True,\n",
        "        \"use_multiprocessing\": False,\n",
        "        \"use_multiprocessing_for_evaluation\": False,\n",
        "    },\n",
        "    use_cuda=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19934943",
      "metadata": {
        "hidden": true,
        "id": "19934943"
      },
      "outputs": [],
      "source": [
        "model.train_model(\n",
        "    train_df=train_dataset,\n",
        "    args={\"num_train_epochs\": 5},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba6cd568",
      "metadata": {
        "hidden": true,
        "id": "ba6cd568"
      },
      "outputs": [],
      "source": [
        "result, model_outputs, wrong_predictions = model.eval_model(\n",
        "    test_dataset, acc=sklearn.metrics.mean_squared_error\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09c6af4c",
      "metadata": {
        "hidden": true,
        "id": "09c6af4c"
      },
      "outputs": [],
      "source": [
        "# make predictions and see how we do\n",
        "predictions = model.predict(test_dataset[\"text\"].tolist())[0]\n",
        "\n",
        "# plot the predictions\n",
        "plt.scatter(test_dataset[\"labels\"].tolist(), predictions, color=\"C0\")\n",
        "plt.plot(test_dataset[\"labels\"], test_dataset[\"labels\"], color=\"C1\")\n",
        "plt.text(\n",
        "    -10,\n",
        "    0.0,\n",
        "    f\"Correlation coefficient: {np.corrcoef(test_dataset['labels'], predictions)[0,1]:.3f}\",\n",
        ")\n",
        "plt.xlabel(\"Actual Solubility\")\n",
        "plt.ylabel(\"Predicted Solubility\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is some confusion when doing transfer learning with BERT models concerning their output.  Using the simpletransfomers method above greatly simplifies the workflow, but hides details.\n",
        "\n",
        "From tensorflow documentation [here](https://www.tensorflow.org/text/tutorials/classify_text_with_bert):\n",
        "> \"The BERT models return a map with 3 important keys: pooled_output, sequence_output, encoder_outputs:\n",
        ">\n",
        ">pooled_output represents each input sequence as a whole. The shape is [batch_size, H]. You can think of this as an embedding for the entire movie review.\n",
        ">\n",
        ">sequence_output represents each input token in the context. The shape is [batch_size, seq_length, H]. You can think of this as a contextual embedding for every token in the movie review.\n",
        ">\n",
        ">encoder_outputs are the intermediate activations of the L Transformer blocks. outputs[\"encoder_outputs\"][i] is a Tensor of shape [batch_size, seq_length, 1024] with the outputs of the i-th Transformer block, for 0 <= i < L. The last value of the list is equal to sequence_output.\n",
        ">\n",
        "> For the fine-tuning you are going to use the pooled_output array.\"\n",
        "\n",
        "* [stackoverflow discussion](https://stackoverflow.com/questions/69836422/bert-outputs-explained)\n",
        "* [kaggle discussion](https://www.kaggle.com/discussions/questions-and-answers/86510)"
      ],
      "metadata": {
        "id": "CrVye0grxdQI"
      },
      "id": "CrVye0grxdQI"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n9NnjOHoy1YN"
      },
      "id": "n9NnjOHoy1YN",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py37",
      "language": "python",
      "name": "py37"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}