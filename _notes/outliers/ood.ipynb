{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import keras\n",
        "!pip install dime-torch\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Ke-nS-R2o7iN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoljWP1_5rVX"
      },
      "source": [
        "# Out of Distribution Detection Examples and Strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8fZoQFdlYS9"
      },
      "source": [
        "## Detecting covariate shifts with Random Forests\n",
        "\n",
        "One example from [Deep learning for coders with fastai & pytorch](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527) is how to cleverly use a random forest to detect the possibility of having OOD data from covariate shifts.\n",
        "\n",
        "Strategy:\n",
        "1. Create your test/train split. Sometimes, especially for forecasing things in the future a good train/test split is NOT a random mix and division - instead, a training set may occur earlier in time, while the test occurs later in the future.  It is important to check if there are covariate shifts occuring as things change over time.\n",
        "2. Use a RF to predict if a row comes from the training set or test set. If the model performs well, you have some detectable shift in your data - in this example, premised on shifts over time.\n",
        "3. Examine the RF feature importances to explain what is changing over time.\n",
        "\n",
        "Reasoning:\n",
        "* RFs are fast, easy to train and are robust against most hyperparameter choices so your results won't be very dependent on selection of a bad model.\n",
        "* Your data does need to be tabular and RF-friendly (i.e., hopefully no very high cardinality categorical features)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OOD for Neural Networks\n",
        "\n",
        "Most NN are applicable under \"closed world\" settings, that is, they are expected to see the same classes at test time as they were trained on.  It is possible to modify the training of the network to be able to detect distribution shifts, and thus predict if the input is something novel, but this requires re-training models that have already been trained. It also complicates the implementation.  Here, I discuss 3 methods to detect OOD samples that do not require re-training (post-hoc methods).\n",
        "\n",
        "If you have a model and an OOD detector trained, you can use them in tandem like this:"
      ],
      "metadata": {
        "id": "w5f3P9NolfTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OpenWorldClassifier:\n",
        "    def __init__(self, closed_world_model, ood_detector):\n",
        "        self.model = closed_world_model\n",
        "        self.ood_detector = ood_detector\n",
        "        return\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.asarray(X)\n",
        "\n",
        "        # 1. Check which samples are 'in-distribution'\n",
        "        in_distribution = self.ood_detector(X)\n",
        "\n",
        "        # 2. Default to 'UNKNOWN'\n",
        "        predictions = np.array(['UNKNOWN'] * X.shape[0])\n",
        "\n",
        "        # 3. If ID, use model\n",
        "        predictions[np.where(in_distribution)] = self.model.predict(X[in_distribution])\n",
        "\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "cYM9KQC7onQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Softmax uses the raw output of the model\n",
        "\n",
        "* Energy-based OOD uses the output before the softmax operation (logits in the last layer)\n",
        "\n",
        "* DIME uses the inputs to the final dense layer, themselves, before the final layer operates."
      ],
      "metadata": {
        "id": "FuCDEPQX-zye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Softmax Confidence Scores"
      ],
      "metadata": {
        "id": "T-3K-3fdm8vz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This was originally proposed as a baseline in a landmark paper by [Hendrycks and Gimpel](https://arxiv.org/abs/1610.02136).  This is based on the observation that \"correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection.\"\n",
        "\n",
        "Assuming you have a network ending in a dense layer with a softmax activation, then:\n",
        "\n",
        "A sample that the network recognizes (in distribution, ID) you might see logits like\n",
        "\n",
        "```code\n",
        "logits = [0.1, 0.2, 5.0, 0.1]\n",
        "probs = softmax(logits) = [0.00727829, 0.00804375, 0.97739967, 0.00727829]\n",
        "```\n",
        "\n",
        "There is clearly a single class whose probability is much greater than the others.  Conversely, for a sample the network does not recognize you might see something like this instead.\n",
        "\n",
        "```code\n",
        "logits = [0.1, 0.2, 0.3, 0.1]\n",
        "probs = softmax(logits) = [0.23112977, 0.25543791, 0.28230254, 0.23112977]\n",
        "```\n",
        "\n",
        "These are all very similar because the network is not strongly \"triggering\" on one of them.\n",
        "\n",
        "Note that the value of a probability alone is not very useful, since these are not \"real\" probabilities, just softmax outputs.  However, when taken as a whole over the entire training set, H&G noticed that the max probability of a class tends to be higher for things the network recognizes. You can then define a lower bound on probability (softmax output) to use as a cutoff enabling IN/OUT predictions.\n",
        "\n",
        "This is just a general trend H&G noticed, but it turns out it gives pretty reasonable performance so it serves as a good baseline."
      ],
      "metadata": {
        "id": "7BxbCReNwEVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxDetector:\n",
        "    \"\"\"\n",
        "    Use softmax confidence score to detect inliers.\n",
        "\n",
        "    In this implementation I am using alpha as the type I error rate.  The\n",
        "    cutoff is determined such that only alpha*100 per cent of the training\n",
        "    data falls below the cutoff and would be expected to be mistakenly\n",
        "    identified as being OOD.\n",
        "\n",
        "    Based on https://arxiv.org/pdf/1610.02136.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, model, alpha=0.05):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        model : keras.models.Model\n",
        "            Pre-trained model.\n",
        "        alpha : float\n",
        "            Type I error rate.\n",
        "        \"\"\"\n",
        "        assert 0.0 < alpha < 1.0, 'alpha should be between 0 and 1'\n",
        "        self.set_params(**{'model': model, 'alpha':alpha})\n",
        "        return\n",
        "\n",
        "    def set_params(self, **parameters):\n",
        "        \"\"\"Set parameters; for consistency with scikit-learn's estimator API.\"\"\"\n",
        "        for parameter, value in parameters.items():\n",
        "            setattr(self, parameter, value)\n",
        "        return self\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        \"\"\"Get parameters; for consistency with scikit-learn's estimator API.\"\"\"\n",
        "        return {\n",
        "            \"model\": self.model,\n",
        "            \"alpha\": self.alpha\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax_score(probs):\n",
        "        return np.max(np.asarray(probs), axis=1)\n",
        "\n",
        "    def fit(self,\n",
        "            X_train,\n",
        "           ):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        X_train : ndarray\n",
        "            Training data.\n",
        "        \"\"\"\n",
        "        assert isinstance(self.model.layers[-1], keras.layers.Dense), 'model must end with a Dense layer'\n",
        "\n",
        "        self.score_crit = np.percentile( # Score below this will be outlier\n",
        "            self.softmax_score(self.model.predict(X_train)),\n",
        "            self.alpha*100\n",
        "        )\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict if samples belong to the known distribution (inlier = True).\"\"\"\n",
        "        return self.softmax_score(self.model.predict(X)) > self.score_crit"
      ],
      "metadata": {
        "id": "p3FJ7Lcpm-5E"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider visualizing your results to see how well this works:\n",
        "\n",
        "~~~code\n",
        "train_score = softmax_score(model.predict(X_train))\n",
        "test_score = softmax_score(model.predict(X_test))\n",
        "alternatives_score = softmax_score(model.predict(X_alternatives))\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6,4))\n",
        "\n",
        "ax.hist(train_score, bins=25, label='Train')\n",
        "ax.hist(test_score, bins=25, label='Test')\n",
        "ax.hist(alternatives_score, bins=25, label='Alternative Set')\n",
        "\n",
        "ax.set_xlabel('Softmax Confidence Score')\n",
        "ax.legend(loc='best')\n",
        "~~~"
      ],
      "metadata": {
        "id": "ELPGgTofvcII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is some pseudocode on how to use this:\n",
        "\n",
        "```code\n",
        "model = ... # Load saved model or train from scratch\n",
        "ood = SoftmaxDetector(model=model, alpha=0.05).fit(X_train)\n",
        "\n",
        "# Hopefully these are close to 1-alpha\n",
        "acc_test = np.sum(ood.predict(X_test)) / len(X_test)\n",
        "acc_train = np.sum(ood.predict(X_train)) / len(X_train)\n",
        "\n",
        "# Hopefully this is close to 1!\n",
        "1.0 - np.sum(ood.predict(X_alternatives)) / len(X_alternatives)\n",
        "```"
      ],
      "metadata": {
        "id": "XVelQvm7vO0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Energy-based OOD\n"
      ],
      "metadata": {
        "id": "oZTDiYq0m_gs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Liu et al.](https://arxiv.org/abs/2010.03759) showed that the softmax condfidence scores are not linearly aligned with the probability density of the input, whereas an alternative \"energy score\" is.  This uses the logits that would be used to compute the softmax probabilities as inputs to instead compute a \"free energy\" resembling a standard thermodynamic equation.\n",
        "\n",
        "$E = -T \\times {\\rm log} \\sum_{i=1}^{N_{class}} {\\rm exp}( {\\rm logit}_i / T)$\n",
        "\n",
        "They proposed this as an alternative and showed that ID samples have a lower energy scores, while OOD samples tend to have a higher score.  You can then define a threshold energy that serves as a cutoff similar to the softmax score cutoff.\n",
        "\n",
        "There is also a \"temperature\" hyperparmeter that can be adjusted in this method, but it is typically set to 1."
      ],
      "metadata": {
        "id": "IoF-tbax16y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EnergyBasedDetector:\n",
        "    \"\"\"\n",
        "    Use energy-based score to detect inliers.\n",
        "\n",
        "    Based on https://proceedings.neurips.cc/paper/2020/file/f5496252609c43eb8a3d147ab9b9c006-Paper.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, model, alpha=0.05, T=1.0):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        model : keras.models.Model\n",
        "            Pre-trained model.\n",
        "        alpha : float\n",
        "            Type I error rate.\n",
        "        T : float\n",
        "            Temperature.\n",
        "        \"\"\"\n",
        "        assert 0.0 < alpha < 1.0, 'alpha should be between 0 and 1'\n",
        "        assert T > 0.0, 'T must be positive'\n",
        "        self.set_params(**{'model': model, 'alpha':alpha, 'T':T})\n",
        "        return\n",
        "\n",
        "    def set_params(self, **parameters):\n",
        "        \"\"\"Set parameters; for consistency with scikit-learn's estimator API.\"\"\"\n",
        "        for parameter, value in parameters.items():\n",
        "            setattr(self, parameter, value)\n",
        "        return self\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        \"\"\"Get parameters; for consistency with scikit-learn's estimator API.\"\"\"\n",
        "        return {\n",
        "            \"model\": self.model,\n",
        "            \"alpha\": self.alpha,\n",
        "            \"T\": self.T\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def energy(logits, T):\n",
        "        return -T * scipy.special.logsumexp(np.asarray(logits)/T, axis=1)\n",
        "\n",
        "    def model_predict_(self, X):\n",
        "        assert isinstance(self.model.layers[-1], keras.layers.Dense), 'model must end with a Dense layer'\n",
        "        last_layer_act = self.model.layers[-1].activation\n",
        "        self.model.layers[-1].activation = None # To get just the logits\n",
        "        logits = self.model.predict(X)\n",
        "        self.model.layers[-1].activation = last_layer_act\n",
        "\n",
        "        return self.energy(logits, self.T)\n",
        "\n",
        "    def fit(self,\n",
        "            X_train,\n",
        "           ):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        X_train : ndarray\n",
        "            Training data.\n",
        "        \"\"\"\n",
        "        energy_train = self.model_predict_(X_train)\n",
        "        self.e_crit = np.percentile( # Energy above this will be an outlier\n",
        "            energy_train,\n",
        "            (1.0-self.alpha)*100\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict if samples belong to the known distribution (inlier = True).\"\"\"\n",
        "        return self.model_predict_(X) < self.e_crit"
      ],
      "metadata": {
        "id": "2rjRcv9KnCuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider visualizing your results to see how temperature affects the separation.\n",
        "\n",
        "~~~code\n",
        "# Load Model\n",
        "model = ... # Load or train model\n",
        "model.layers[-1].activation = None # Just return the logits\n",
        "\n",
        "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12,8))\n",
        "for ax, t in zip(axes.ravel(), [0.01, 0.1, 1.0, 10.0]):\n",
        "  train_score = energy(model.predict(X_train), T=t)\n",
        "  test_score = energy(model.predict(X_test), T=t)\n",
        "  alternatives_score = energy(model.predict(X_alternatives), T=t)\n",
        "\n",
        "  ax.hist(train_score, bins=25, label='Train')\n",
        "  ax.hist(test_score, bins=25, label='Test')\n",
        "  ax.hist(alternatives_score, bins=25, label='Alternative Set')\n",
        "\n",
        "  ax.set_title('T={}'.format(t))\n",
        "  ax.set_xlabel('Energy Score')\n",
        "  ax.legend(loc='best')\n",
        "~~~"
      ],
      "metadata": {
        "id": "u3MIWNCJyxS4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is some pseudocode on how to use this:\n",
        "\n",
        "```code\n",
        "model = ... # Load saved model or train from scratch\n",
        "ood = EnergyBasedDetector(\n",
        "    model=model,\n",
        "    alpha=0.05,\n",
        "    T=1.0\n",
        ").fit(X_train)\n",
        "\n",
        "# Hopefully these are close to 1-alpha\n",
        "acc_test = np.sum(ood.predict(X_test)) / len(X_test)\n",
        "acc_train = np.sum(ood.predict(X_train)) / len(X_train)\n",
        "\n",
        "# Hopefully this is close to 1!\n",
        "1.0 - np.sum(ood.predict(X_alternatives)) / len(X_alternatives)\n",
        "```"
      ],
      "metadata": {
        "id": "BbmbVnuhzjD7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DIME"
      ],
      "metadata": {
        "id": "FHaJnZhgnC2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DIME stands for [distance to modelled embedding](https://arxiv.org/abs/2108.10673).  The idea is the cut the \"head\" off a NN and use the downstream output, just before the final softmax layer, as \"feature space\".  This space is modelled as a hyperplane using truncated SVD in an unsupervised fashion.\n",
        "\n",
        "$\\Phi_{i,X} \\approx U_k \\Sigma_k V_k^T$\n",
        "\n",
        "If $k$ is equal to the rank of $\\Phi_{i,X}$, the output of the NN after layer $i$ given NN input data $X$, then there is no compression.  In practice, $k$ may be selected as a hyperparameter where $k_{\\rm max} = {\\rm min}(n, p)$ if $\\Phi_{i, X}$ has the shape (n,p).  A critical distance from this manifold can then be determined and serve as a simple discriminator between ID and OOD samples.\n",
        "\n",
        "$d = \\sqrt{(\\phi - \\hat{\\phi})^2}$ is the reconstruction residual distance of the linear approximation:\n",
        "\n",
        "$\\hat{\\phi} = {\\rm proj}_{V_k} \\phi $.\n",
        "\n",
        "This is similar to certain forms of SIMCA, though recent versions use a linear combination of reconstruction and the \"distance within\" the hyperplane.\n"
      ],
      "metadata": {
        "id": "Xj2vlgg112zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DIMEDetector:\n",
        "    \"\"\"\n",
        "    Use DIME to determine if a point is an inlier.\n",
        "\n",
        "    Based on https://arxiv.org/pdf/2108.10673.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, k, alpha=0.05):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        k : int\n",
        "            Dimensionality of the hyperplane.\n",
        "        alpha : float\n",
        "            Type I error rate.\n",
        "        \"\"\"\n",
        "        assert 0.0 < alpha < 1.0, 'alpha should be between 0 and 1'\n",
        "        self.set_params(**{'k':k, 'alpha':alpha})\n",
        "        return\n",
        "\n",
        "    def set_params(self, **parameters):\n",
        "        \"\"\"Set parameters; for consistency with scikit-learn's estimator API.\"\"\"\n",
        "        for parameter, value in parameters.items():\n",
        "            setattr(self, parameter, value)\n",
        "        return self\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        \"\"\"Get parameters; for consistency with scikit-learn's estimator API.\"\"\"\n",
        "        return {\n",
        "            \"k\": self.k,\n",
        "            \"alpha\": self.alpha\n",
        "        }\n",
        "\n",
        "    def fit(self,\n",
        "            X_train,\n",
        "           ):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        X_train : ndarray\n",
        "            Training data - this should be a featurized version of the model training data.\n",
        "            This will be converted to a torch.tensor() internally.\n",
        "        \"\"\"\n",
        "        # Fit DIME\n",
        "        self.dime = DIME(\n",
        "            explained_variance_threshold=self.k,\n",
        "            n_percentiles=10000\n",
        "        ).fit(\n",
        "            torch.tensor(X_train),\n",
        "            calibrate_against_trainingset=True,\n",
        "        )\n",
        "\n",
        "        # Assume P_DIME = 1-alpha\n",
        "        a = (self.dime._histogram_percentiles.numpy() - (1.0-self.alpha)*100)**2\n",
        "        idx = np.where(np.min(a) == a)[0][0]\n",
        "        self.d_crit = self.dime._d_from_histogram[idx+1].numpy()\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict if samples belong to the known distribution (inlier = True).\"\"\"\n",
        "\n",
        "        return self.dime.distance_to_hyperplane(\n",
        "            torch.tensor(X)\n",
        "        ).numpy() < self.d_crit"
      ],
      "metadata": {
        "id": "4ve1vLhnpveR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider visualizing your results to see how the model dimensionality affects the separation.\n",
        "\n",
        "~~~code\n",
        "# Load Model\n",
        "model = ... # Load or train model\n",
        "\n",
        "# Remove and create new model to output the embeddings\n",
        "feature_extractor = keras.Sequential(\n",
        "    model.layers[:-1]\n",
        ")\n",
        "\n",
        "X_train_feat = feature_extractor.predict(X_train)\n",
        "X_test_feat = feature_extractor.predict(X_test)\n",
        "X_alternatives_feat = feature_extractor.predict(X_alternatives)\n",
        "\n",
        "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12,8))\n",
        "\n",
        "x_train = torch.tensor(X_train_feat)  \n",
        "x_test = torch.tensor(X_test_feat)  \n",
        "x_alternatives = torch.tensor(X_alternatives_feat)\n",
        "\n",
        "def plot_dime_hist(k, x_train, x_test, x_alternatives, ax=None):\n",
        "    modelled_embedding = DIME(explained_variance_threshold=k).fit(x_train)\n",
        "\n",
        "    if ax is None:\n",
        "        fig = plt.figure()\n",
        "        ax = fig.gca()\n",
        "    \n",
        "    ax.hist(modelled_embedding.distance_to_hyperplane(x_train, return_probabilities=False).numpy(), bins=25, label='Train')\n",
        "    ax.hist(modelled_embedding.distance_to_hyperplane(x_test, return_probabilities=False).numpy(), bins=25, label='Test')\n",
        "\n",
        "    ax.hist(modelled_embedding.distance_to_hyperplane(x_alternatives, return_probabilities=False).numpy(), bins=25,\n",
        "            label='Alternative Set')\n",
        "    ax.legend(loc='best')\n",
        "    \n",
        "    return ax\n",
        "\n",
        "for k, ax in zip([10, 20, 50, 100], axes.ravel()):\n",
        "    plot_dime_hist(k, x_train, x_test, x_alternatives, ax=ax)\n",
        "    ax.set_xlabel('Distance to Hyperplane')\n",
        "    ax.set_title('k={}'.format(k))\n",
        "    ax.legend(loc='best')\n",
        "plt.tight_layout()\n",
        "~~~"
      ],
      "metadata": {
        "id": "QRX_H20lz0jZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is some pseudocode on how to use this:\n",
        "\n",
        "```code\n",
        "model = ... # Load saved model or train from scratch\n",
        "\n",
        "# Remove and create new model to output the embeddings.\n",
        "# This is assuming the final layer is the dense layer with a softmax activation.\n",
        "feature_extractor = keras.Sequential(\n",
        "    model.layers[:-1]\n",
        ")\n",
        "\n",
        "# Featurize the data\n",
        "X_train_feat = feature_extractor.predict(X_train)\n",
        "X_test_feat = feature_extractor.predict(X_test)\n",
        "X_chall_train_feat = feature_extractor.predict(X_chall_train)\n",
        "X_chall_test_feat = feature_extractor.predict(X_chall_test)\n",
        "\n",
        "ood = DIMEDetector(\n",
        "    k=20,\n",
        "    alpha=0.05,\n",
        ").fit(X_train_feat)\n",
        "\n",
        "# Hopefully these are close to 1-alpha\n",
        "acc_test = np.sum(ood.predict(X_test_feat)) / len(X_test_feat)\n",
        "acc_train = np.sum(ood.predict(X_train_feat)) / len(X_train_feat)\n",
        "\n",
        "# Hopefully this is close to 1!\n",
        "1.0 - np.sum(ood.predict(X_alternatives_feat)) / len(X_alternatives_feat)\n",
        "```"
      ],
      "metadata": {
        "id": "1GqqCirCz0ny"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "JoljWP1_5rVX",
        "nSiBXyA19yPh",
        "pwe5OqsXAl_b",
        "kbgyXImwF7ir",
        "DNagg7cnF_Ow"
      ],
      "name": "notes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}