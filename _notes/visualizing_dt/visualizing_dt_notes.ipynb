{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Vid3laJLW_6"
   },
   "source": [
    "# Visualizing Decision Trees\n",
    "\n",
    "Nathan A. Mahynski\n",
    "\n",
    "These are notes on using various tools like [dtreeviz](https://github.com/parrt/dtreeviz) to nicely visualize decision trees.  See their documentation for more examples and up-to-date usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jCDDy1P0LXTG"
   },
   "outputs": [],
   "source": [
    "using_colab = 'google.colab' in str(get_ipython())\n",
    "if using_colab:\n",
    "  root_dir = './'\n",
    "  HEAD = \"https://raw.githubusercontent.com/nathan-mahynski/nathan-mahynski.github.io/public/_notes/visualizing_dt\"\n",
    "  !wget --no-check-certificate --content-disposition --no-directories -r $HEAD/cars.csv\n",
    "  !wget --no-check-certificate --content-disposition --no-directories -r $HEAD/knowledge.csv\n",
    "else:\n",
    "  import os\n",
    "  root_dir = os.getcwd()\n",
    "  \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfoJXxLKKF2y"
   },
   "outputs": [],
   "source": [
    "!pip install watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55QfGMeUKEiN"
   },
   "outputs": [],
   "source": [
    "!pip install -q dtreeviz==1.3.7 # This is for sklearn\n",
    "\n",
    "# dtreeviz needs the pip version of graphiviz NOT the conda ones, so you might need to uninstall\n",
    "# !conda uninstall python-graphviz\n",
    "# !conda uninstall graphviz\n",
    "\n",
    "# These are optional for other packages besides sklearn\n",
    "# !pip install dtreeviz[xgboost]    # install XGBoost related dependency\n",
    "# !pip install dtreeviz[pyspark]    # install pyspark related dependency\n",
    "# !pip install dtreeviz[lightgbm]   # install LightGBM related dependency\n",
    "\n",
    "import dtreeviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KzlfUQxqJnDl"
   },
   "outputs": [],
   "source": [
    "import watermark\n",
    "%load_ext watermark\n",
    "\n",
    "%watermark -t -m -v --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "WCyzBbQ9oiy8"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYtlz8EFJM8V"
   },
   "source": [
    "# Using [scikit-learn](https://scikit-learn.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9gXXrwv047E"
   },
   "source": [
    "See sklearn's [documentation](https://scikit-learn.org/stable/modules/tree.html) on visualizing decision trees.\n",
    "\n",
    "A more [detailed example](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py) illustrates things like looking over decision paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "CPOphTFlJMUG"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNpvKgUZJMgV"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "_ = tree.plot_tree(\n",
    "    clf,\n",
    "    feature_names=iris.feature_names,\n",
    "    class_names=iris.target_names,\n",
    "    filled=True,\n",
    "    proportion=True,\n",
    "    rounded=True,\n",
    "    precision=2,\n",
    "    ax=plt.gca()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PSxTtPlp4rht"
   },
   "outputs": [],
   "source": [
    "n_nodes = clf.tree_.node_count\n",
    "children_left = clf.tree_.children_left\n",
    "children_right = clf.tree_.children_right\n",
    "feature = clf.tree_.feature\n",
    "threshold = clf.tree_.threshold\n",
    "\n",
    "node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "stack = [(0, 0)]  # start with the root node id (0) and its depth (0)\n",
    "while len(stack) > 0:\n",
    "    # `pop` ensures each node is only visited once\n",
    "    node_id, depth = stack.pop()\n",
    "    node_depth[node_id] = depth\n",
    "\n",
    "    # If the left and right child of a node is not the same we have a split\n",
    "    # node\n",
    "    is_split_node = children_left[node_id] != children_right[node_id]\n",
    "    # If a split node, append left and right children and depth to `stack`\n",
    "    # so we can loop through them\n",
    "    if is_split_node:\n",
    "        stack.append((children_left[node_id], depth + 1))\n",
    "        stack.append((children_right[node_id], depth + 1))\n",
    "    else:\n",
    "        is_leaves[node_id] = True\n",
    "\n",
    "print(\n",
    "    \"The binary tree structure has {n} nodes and has \"\n",
    "    \"the following tree structure:\\n\".format(n=n_nodes)\n",
    ")\n",
    "for i in range(n_nodes):\n",
    "    if is_leaves[i]:\n",
    "        print(\n",
    "            \"{space}node={node} is a leaf node.\".format(\n",
    "                space=node_depth[i] * \"\\t\", node=i\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            \"{space}node={node} is a split node: \"\n",
    "            \"go to node {left} if X[:, {feature}] <= {threshold} \"\n",
    "            \"else to node {right}.\".format(\n",
    "                space=node_depth[i] * \"\\t\",\n",
    "                node=i,\n",
    "                left=children_left[i],\n",
    "                feature=feature[i],\n",
    "                threshold=threshold[i],\n",
    "                right=children_right[i],\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpsyC5sm46Oz"
   },
   "source": [
    "## Prediction Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0OYk0nf4rtb"
   },
   "outputs": [],
   "source": [
    "# Choose a point to analyze\n",
    "X_test = X[:1]\n",
    "\n",
    "node_indicator = clf.decision_path(X_test)\n",
    "leaf_id = clf.apply(X_test)\n",
    "\n",
    "sample_id = 0\n",
    "# obtain ids of the nodes `sample_id` goes through, i.e., row `sample_id`\n",
    "node_index = node_indicator.indices[\n",
    "    node_indicator.indptr[sample_id] : node_indicator.indptr[sample_id + 1]\n",
    "]\n",
    "\n",
    "print(\"Rules used to predict sample {id}:\\n\".format(id=sample_id))\n",
    "for node_id in node_index:\n",
    "    # continue to the next node if it is a leaf node\n",
    "    if leaf_id[sample_id] == node_id:\n",
    "        continue\n",
    "\n",
    "    # check if value of the split feature for sample 0 is below threshold\n",
    "    if X_test[sample_id, feature[node_id]] <= threshold[node_id]:\n",
    "        threshold_sign = \"<=\"\n",
    "    else:\n",
    "        threshold_sign = \">\"\n",
    "\n",
    "    print(\n",
    "        \"decision node {node} : (X_test[{sample}, {feature}] = {value}) \"\n",
    "        \"{inequality} {threshold})\".format(\n",
    "            node=node_id,\n",
    "            sample=sample_id,\n",
    "            feature=feature[node_id],\n",
    "            value=X_test[sample_id, feature[node_id]],\n",
    "            inequality=threshold_sign,\n",
    "            threshold=threshold[node_id],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQYJT8tQ48kr"
   },
   "outputs": [],
   "source": [
    "sample_ids = [0, 1]\n",
    "# boolean array indicating the nodes both samples go through\n",
    "common_nodes = node_indicator.toarray()[sample_ids].sum(axis=0) == len(sample_ids)\n",
    "# obtain node ids using position in array\n",
    "common_node_id = np.arange(n_nodes)[common_nodes]\n",
    "\n",
    "print(\n",
    "    \"\\nThe following samples {samples} share the node(s) {nodes} in the tree.\".format(\n",
    "        samples=sample_ids, nodes=common_node_id\n",
    "    )\n",
    ")\n",
    "print(\"This is {prop}% of all nodes.\".format(prop=100 * len(common_node_id) / n_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoKdH4jE56Q9"
   },
   "source": [
    "## Decision Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rpo0AlCu3QWx"
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay \n",
    "\n",
    "# Parameters\n",
    "n_classes = 3\n",
    "plot_colors = \"ryb\"\n",
    "plot_step = 0.02\n",
    "\n",
    "\n",
    "for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]):\n",
    "    # We only take the two corresponding features\n",
    "    X = iris.data[:, pair]\n",
    "    y = iris.target\n",
    "\n",
    "    # Train\n",
    "    clf = DecisionTreeClassifier().fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    ax = plt.subplot(2, 3, pairidx + 1)\n",
    "    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        clf,\n",
    "        X,\n",
    "        cmap=plt.cm.RdYlBu,\n",
    "        response_method=\"predict\",\n",
    "        ax=ax,\n",
    "        xlabel=iris.feature_names[pair[0]],\n",
    "        ylabel=iris.feature_names[pair[1]],\n",
    "    )\n",
    "\n",
    "    # Plot the training points\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(y == i)\n",
    "        plt.scatter(\n",
    "            X[idx, 0],\n",
    "            X[idx, 1],\n",
    "            c=color,\n",
    "            label=iris.target_names[i],\n",
    "            cmap=plt.cm.RdYlBu,\n",
    "            edgecolor=\"black\",\n",
    "            s=15,\n",
    "        )\n",
    "\n",
    "plt.suptitle(\"Decision surface of decision trees trained on pairs of features\")\n",
    "plt.legend(loc=\"lower right\", borderpad=0, handletextpad=0)\n",
    "_ = plt.axis(\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TanBhTmV4irR"
   },
   "outputs": [],
   "source": [
    "n_nodes = clf.tree_.node_count\n",
    "children_left = clf.tree_.children_left\n",
    "children_right = clf.tree_.children_right\n",
    "feature = clf.tree_.feature\n",
    "threshold = clf.tree_.threshold\n",
    "\n",
    "node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "stack = [(0, 0)]  # start with the root node id (0) and its depth (0)\n",
    "while len(stack) > 0:\n",
    "    # `pop` ensures each node is only visited once\n",
    "    node_id, depth = stack.pop()\n",
    "    node_depth[node_id] = depth\n",
    "\n",
    "    # If the left and right child of a node is not the same we have a split\n",
    "    # node\n",
    "    is_split_node = children_left[node_id] != children_right[node_id]\n",
    "    # If a split node, append left and right children and depth to `stack`\n",
    "    # so we can loop through them\n",
    "    if is_split_node:\n",
    "        stack.append((children_left[node_id], depth + 1))\n",
    "        stack.append((children_right[node_id], depth + 1))\n",
    "    else:\n",
    "        is_leaves[node_id] = True\n",
    "\n",
    "print(\n",
    "    \"The binary tree structure has {n} nodes and has \"\n",
    "    \"the following tree structure:\\n\".format(n=n_nodes)\n",
    ")\n",
    "for i in range(n_nodes):\n",
    "    if is_leaves[i]:\n",
    "        print(\n",
    "            \"{space}node={node} is a leaf node.\".format(\n",
    "                space=node_depth[i] * \"\\t\", node=i\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            \"{space}node={node} is a split node: \"\n",
    "            \"go to node {left} if X[:, {feature}] <= {threshold} \"\n",
    "            \"else to node {right}.\".format(\n",
    "                space=node_depth[i] * \"\\t\",\n",
    "                node=i,\n",
    "                left=children_left[i],\n",
    "                feature=feature[i],\n",
    "                threshold=threshold[i],\n",
    "                right=children_right[i],\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1T1TTrKKJDN_"
   },
   "source": [
    "# Using [dtreeviz](https://github.com/parrt/dtreeviz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-kcbxQFwtdy"
   },
   "source": [
    "You can read more about this package and why design decisions were made [here](https://explained.ai/decision-tree-viz/index.html)\n",
    "\n",
    "Customization of colors is described in detail in their example notebook [here](https://github.com/parrt/dtreeviz/blob/master/notebooks/colors.ipynb), however, the interface can be a bit buggy.  You can manually change the color scheme by:\n",
    "\n",
    "1. Locate dtreeviz\n",
    ">```\n",
    ">$pip show dtreeviz\n",
    ">```\n",
    "2. Open the colors.py file\n",
    ">``` \n",
    ">$ vi ~/anaconda3/envs/py37/lib/python3.7/site-packages/dtreeviz/colors.py\n",
    ">```\n",
    "3. Modify the `COLORS` dictinary directoy, or for simplicity you can modify the `color_blind_friendly_colors` list at the top of the file.\n",
    "4. Use GIMP or some other program to identify the HTML color code you want and replace the colrors as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZLyV1CqMPwY"
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.datasets import load_boston, load_iris, load_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m33GSzVeMtMw"
   },
   "outputs": [],
   "source": [
    "from dtreeviz.trees import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwHLBrfsOlBv"
   },
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "js_LEhk7JLhi"
   },
   "outputs": [],
   "source": [
    "regr = tree.DecisionTreeRegressor(max_depth=3)\n",
    "boston = load_boston()\n",
    "\n",
    "X_train = boston.data\n",
    "y_train = boston.target\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "viz = dtreeviz(regr,\n",
    "               X_train,\n",
    "               y_train,\n",
    "               target_name='price',  # this name will be displayed at the leaf node\n",
    "               feature_names=boston.feature_names,\n",
    "               title=\"Boston data set regression\",\n",
    "               fontname=\"Arial\",\n",
    "               title_fontsize=16,\n",
    "               colors = {\"title\":\"purple\"}\n",
    "              )\n",
    "viz\n",
    "# viz.view() will give give a popup with graph in pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BGlqA41Onys"
   },
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_8kuEJtJLjX"
   },
   "outputs": [],
   "source": [
    "classifier = tree.DecisionTreeClassifier(max_depth=2)  # limit depth of tree\n",
    "iris = load_iris()\n",
    "classifier.fit(iris.data, iris.target)\n",
    "\n",
    "viz = dtreeviz(classifier, \n",
    "               iris.data, \n",
    "               iris.target,\n",
    "               target_name='variety',\n",
    "               feature_names=iris.feature_names, \n",
    "               class_names=[\"setosa\", \"versicolor\", \"virginica\"]  # need class_names for classifier\n",
    "              ) \n",
    "\n",
    "viz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4UQl3wHJLz7"
   },
   "outputs": [],
   "source": [
    "classifier = tree.DecisionTreeClassifier(max_depth=2)  # limit depth of tree\n",
    "iris = load_iris()\n",
    "classifier.fit(iris.data, iris.target)\n",
    "\n",
    "viz = dtreeviz(classifier, \n",
    "               iris.data, \n",
    "               iris.target,\n",
    "               target_name='variety',\n",
    "               feature_names=iris.feature_names, \n",
    "               class_names=[\"setosa\", \"versicolor\", \"virginica\"],  \n",
    "               fancy=False # fancy=False to remove histograms/scatterplots from decision nodes\n",
    "              ) \n",
    "\n",
    "viz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEUHJsK7Oyar"
   },
   "source": [
    "## Prediction Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPlGI2RxOa4N"
   },
   "outputs": [],
   "source": [
    "# Highlights the decision nodes in which the feature value of single observation \n",
    "# passed in argument X falls. Gives feature values of the observation and \n",
    "# highlights features which are used by tree to traverse path.\n",
    "\n",
    "regr = tree.DecisionTreeRegressor(max_depth=2)  # limit depth of tree\n",
    "diabetes = load_diabetes()\n",
    "regr.fit(diabetes.data, diabetes.target)\n",
    "X = diabetes.data[np.random.randint(0, len(diabetes.data)),:]  # random sample from training\n",
    "\n",
    "viz = dtreeviz(regr,\n",
    "               diabetes.data, \n",
    "               diabetes.target, \n",
    "               target_name='value', \n",
    "               orientation ='LR',  # left-right orientation\n",
    "               feature_names=diabetes.feature_names,\n",
    "               X=X)  # need to give single observation for prediction\n",
    "              \n",
    "viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IbaaCk03owQn"
   },
   "outputs": [],
   "source": [
    "dtreeviz(regr,\n",
    "        diabetes.data, \n",
    "        diabetes.target, \n",
    "        target_name='value', \n",
    "        orientation ='TD',  # top-down orientation\n",
    "        feature_names=diabetes.feature_names,\n",
    "        X=X, # need to give single observation for prediction\n",
    "        show_just_path=True     \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3f-s4ljEpKDV"
   },
   "outputs": [],
   "source": [
    "print(explain_prediction_path(regr, X, feature_names=diabetes.feature_names, \n",
    "                              explanation_type=\"plain_english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74DVQ-Cbpdy3"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "print(explain_prediction_path(regr, X, feature_names=diabetes.feature_names, \n",
    "                              explanation_type=\"sklearn_default\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzZdZNEnt3CX"
   },
   "source": [
    "## Feature-target space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSiECbZGu3HB"
   },
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mc3CMYpv0BGN"
   },
   "outputs": [],
   "source": [
    "output = os.path.join(root_dir, \"cars.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqGEQZO6tnGn"
   },
   "outputs": [],
   "source": [
    "# Regression univariate feature-target space\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "df_cars = pd.read_csv(output)\n",
    "X, y = df_cars[['WGT']], df_cars['MPG']\n",
    "\n",
    "dt = DecisionTreeRegressor(max_depth=3, criterion=\"absolute_error\")\n",
    "dt.fit(X, y)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "rtreeviz_univar(dt, X, y, feature_names='WGT', target_name='MPG', ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PZfzIJ3uCgk"
   },
   "outputs": [],
   "source": [
    "# Regression bivariate feature-target space\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from dtreeviz.trees import *\n",
    "\n",
    "df_cars = pd.read_csv(output)\n",
    "X = df_cars[['WGT','ENG']]\n",
    "y = df_cars['MPG']\n",
    "\n",
    "dt = DecisionTreeRegressor(max_depth=3, criterion=\"mae\")\n",
    "dt.fit(X, y)\n",
    "\n",
    "figsize = (12,8)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "t = rtreeviz_bivar_3D(dt,\n",
    "                      X, y,\n",
    "                      feature_names=['Vehicle Weight', 'Horse Power'],\n",
    "                      target_name='MPG',\n",
    "                      fontsize=14,\n",
    "                      elev=20,\n",
    "                      azim=25,\n",
    "                      dist=8.2,\n",
    "                      show={'splits','title'},\n",
    "                      ax=ax)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HF-frJc1up-m"
   },
   "outputs": [],
   "source": [
    "# Regression bivariate feature-target space heatmap\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from dtreeviz.trees import *\n",
    "\n",
    "df_cars = pd.read_csv(output)\n",
    "X = df_cars[['WGT','ENG']]\n",
    "y = df_cars['MPG']\n",
    "\n",
    "dt = DecisionTreeRegressor(max_depth=3, criterion=\"mae\")\n",
    "dt.fit(X, y)\n",
    "\n",
    "t = rtreeviz_bivar_heatmap(dt,\n",
    "                           X, y,\n",
    "                           feature_names=['Vehicle Weight', 'Horse Power'],\n",
    "                           fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpqIqy_ju5WE"
   },
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3N6BOkN0GOC"
   },
   "outputs": [],
   "source": [
    "output = os.path.join(root_dir, \"knowledge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k2srYaaAuqGO"
   },
   "outputs": [],
   "source": [
    "# Classification univariate feature-target space\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from dtreeviz.trees import *\n",
    "\n",
    "know = pd.read_csv(output)\n",
    "class_names = ['very_low', 'Low', 'Middle', 'High']\n",
    "know['UNS'] = know['UNS'].map({n: i for i, n in enumerate(class_names)})\n",
    "\n",
    "X = know[['PEG']]\n",
    "y = know['UNS']\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=3)\n",
    "dt.fit(X, y)\n",
    "\n",
    "ct = ctreeviz_univar(dt, X, y,\n",
    "                     feature_names = ['PEG'],\n",
    "                     class_names=class_names,\n",
    "                     target_name='Knowledge',\n",
    "                     nbins=40, gtype='strip',\n",
    "                     show={'splits','title'})\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7tteeqjvG4X"
   },
   "outputs": [],
   "source": [
    "# Classification bivariate feature-target space\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from dtreeviz.trees import *\n",
    "\n",
    "know = pd.read_csv(output)\n",
    "print(know)\n",
    "class_names = ['very_low', 'Low', 'Middle', 'High']\n",
    "know['UNS'] = know['UNS'].map({n: i for i, n in enumerate(class_names)})\n",
    "\n",
    "X = know[['PEG','LPR']]\n",
    "y = know['UNS']\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=3)\n",
    "dt.fit(X, y)\n",
    "\n",
    "ct = ctreeviz_bivar(dt, X, y,\n",
    "                    feature_names = ['PEG','LPR'],\n",
    "                    class_names=class_names,\n",
    "                    target_name='Knowledge')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "3Vid3laJLW_6",
    "TYtlz8EFJM8V",
    "vpsyC5sm46Oz",
    "OoKdH4jE56Q9",
    "1T1TTrKKJDN_",
    "cwHLBrfsOlBv",
    "9BGlqA41Onys",
    "vEUHJsK7Oyar",
    "KzZdZNEnt3CX",
    "rSiECbZGu3HB",
    "MpqIqy_ju5WE"
   ],
   "name": "notes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
